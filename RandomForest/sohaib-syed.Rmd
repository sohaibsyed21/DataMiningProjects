---
title: "CS 422- Homework 6"
output: 
  html_notebook:
    toc: yes
    toc_float: yes
author: Sohaib Syed, A20439074
---
### Part 2.1-A
```{r}
library("ISLR")
library(rpart)
library(rpart.plot)
library(randomForest)
library(caret)

df<-read.csv(file="hotel_bookings.csv")
set.seed(1122)
index <- sample(1:nrow(df), .90*dim(df)[1])
train.df <-df[index,]
test.df <-df[-index,]


train.df$is_canceled <- as.character(train.df$is_canceled)
train.df$is_canceled<-as.factor(train.df$is_canceled)
test.df$is_canceled <- as.character(test.df$is_canceled)

error_vec<-c()
pred_vec<-c()
for (a in c(2,3,4)){
  for(b in c(250,500,750)){
    model<-randomForest(is_canceled~lead_time+market_segment+distribution_channel+previous_cancellations+deposit_type+customer_type+required_car_parking_spaces+total_of_special_requests,data=train.df,ntree=b,mtry=a)
    error_vec<-c(error_vec,mean(model$err.rate))
    pred<-predict(model, test.df, type="class")
    pred_vec<-c(pred_vec,confusionMatrix(pred,as.factor(test.df$is_canceled), positive='1'),c(a,b))
    rm(model,pred)
  }
}
error_vec
pred_vec
cat("Grid search resulted in the best model at ntree = 500 and mtry = 4.\nAccuracy: 0.817\nBalanced Accuracy: 0.718\nSensitivity: 0.640\nSpecificity: 0.923\n  ")
```
### Part 2.1-B
```{r}
cat("Grid search resulted in the best model for OOB at ntree = 750 and mtry = 4.\nOOB=.2067")

```
### Part 2.1-C
```{r}
#The best models are different in part a and part b. As the trees get larger and are trained to be deeper the OOB gets smaller because the data becomes over-fitted. That is why the largest tree had the lowest OOB error, but it did not have the largest balanced accuracy. The tree in part a was smaller and better at generalizing to the test data, but was still susceptible to random points being mis-classified. 
```