train.df <-Auto[index,]
test.df <-Auto [-index,]
train_model<- lm(mpg ~. -name,data=train.df)
summary(train_model)
RSS<- c(crossprod(train_model$residuals))
MSE <- RSS/ length(train_model$residuals)
RMSE <- sqrt(MSE)
paste("R2 = .8135, RSE =  3.367, RMSE= ", RMSE)
# The model fits the data well because we see that the R2 value is high, which indicates a strong fit. RSE is also low which means that the residual error doesn't vary a lot, and similar with RMSE which means that the average residual error also doesn't vary as much. RSE and RMSE are close to each other which i will infer as it meaning that between the different residuals the average and error don't deviate a lot. Essentially, these are signs of a model with a good fit.
plot(train_model$residuals)
hist(train_model$residuals,main="Histogram of training model residuals", xlab="Residuals")
# The distribution is very close to a Gaussian one. It is very slightly skewed to the left. What I can say about the residuals is that most of them are very close to the mean.
# From part a-ii, the most significant predictors are weight, year, and origin.
new_train_model<- lm(mpg ~ weight+year+origin,data=train.df)
summary(new_train_model)
new_RSS<- c(crossprod(new_train_model$residuals))
new_MSE <- new_RSS/ length(new_train_model$residuals)
new_RMSE <- sqrt(new_MSE)
paste("R2 = .8111, RSE =  3.389, RMSE= ", new_RMSE)
# This model fits the data pretty well as the R2 is still high, the RSE and RMSE are pretty close to 0.
plot(new_train_model$residuals)
hist(new_train_model$residuals,main="Histogram of new training model residuals", xlab="Residuals")
library("ISLR")
set.seed(1122)
index <- sample(1:nrow(Auto), .95*dim(Auto)[1])
train.df <-Auto[index,]
test.df <-Auto [-index,]
train_model<- lm(mpg ~. -name,data=train.df)
summary(train_model)
RSS<- c(crossprod(train_model$residuals))
MSE <- RSS/ length(train_model$residuals)
RMSE <- sqrt(MSE)
paste("R2 = .8135, RSE =  3.367, RMSE= ", RMSE)
# The model fits the data well because we see that the R2 value is high, which indicates a strong fit. RSE is also low which means that the residual error doesn't vary a lot, and similar with RMSE which means that the average residual error also doesn't vary as much. RSE and RMSE are close to each other which i will infer as it meaning that between the different residuals the average and error don't deviate a lot. Essentially, these are signs of a model with a good fit.
plot(train_model$residuals)
hist(train_model$residuals,main="Histogram of training model residuals", xlab="Residuals")
# The distribution is very close to a Gaussian one. It is very slightly skewed to the left. What I can say about the residuals is that most of them are very close to the mean.
# From part a-ii, the most significant predictors are weight, year, and origin.
new_train_model<- lm(mpg ~ weight+year+origin,data=train.df)
summary(new_train_model)
new_RSS<- c(crossprod(new_train_model$residuals))
new_MSE <- new_RSS/ length(new_train_model$residuals)
new_RMSE <- sqrt(new_MSE)
paste("R2 = .8111, RSE =  3.389, RMSE= ", new_RMSE)
# This model fits the data pretty well as the R2 is still high, the RSE and RMSE are pretty close to 0.
plot(new_train_model$residuals)
hist(new_train_model$residuals,main="Histogram of new training model residuals", xlab="Residuals")
prediction_model<-predict(new_train_model)
summary(prediction_model)
new_prediction<-predict(new_train_model,test.df)
new_prediction<-predict(new_train_model,test.df)
summary(new_prediction)
new_prediction<-predict(new_train_model,test.df)
new_prediction
columnn<-c(1,1,3,4,5,)
columnn<-c(1,1,3,4,5)
columnn
columnn<-r(1,2,3)
columnn<-row(1,2,3)
columnn<-c(predict(new_train_model,test.df),test.df$mpg)
columnn
library("ISLR")
set.seed(1122)
index <- sample(1:nrow(Auto), .95*dim(Auto)[1])
train.df <-Auto[index,]
test.df <-Auto [-index,]
train_model<- lm(mpg ~. -name,data=train.df)
summary(train_model)
RSS<- c(crossprod(train_model$residuals))
MSE <- RSS/ length(train_model$residuals)
RMSE <- sqrt(MSE)
paste("R2 = .8135, RSE =  3.367, RMSE= ", RMSE)
# The model fits the data well because we see that the R2 value is high, which indicates a strong fit. RSE is also low which means that the residual error doesn't vary a lot, and similar with RMSE which means that the average residual error also doesn't vary as much. RSE and RMSE are close to each other which i will infer as it meaning that between the different residuals the average and error don't deviate a lot. Essentially, these are signs of a model with a good fit.
plot(train_model$residuals)
hist(train_model$residuals,main="Histogram of training model residuals", xlab="Residuals")
# The distribution is very close to a Gaussian one. It is very slightly skewed to the left. What I can say about the residuals is that most of them are very close to the mean.
# From part a-ii, the most significant predictors are weight, year, and origin.
new_train_model<- lm(mpg ~ weight+year+origin,data=train.df)
summary(new_train_model)
new_RSS<- c(crossprod(new_train_model$residuals))
new_MSE <- new_RSS/ length(new_train_model$residuals)
new_RMSE <- sqrt(new_MSE)
paste("R2 = .8111, RSE =  3.389, RMSE= ", new_RMSE)
# This model fits the data pretty well as the R2 is still high, the RSE and RMSE are pretty close to 0.
plot(new_train_model$residuals)
hist(new_train_model$residuals,main="Histogram of new training model residuals", xlab="Residuals")
# There is a Gaussian distribution and it looks like this time there are more residuals closer to positive 15. There also seems to be less negative residuals.
prediction<-c(predict(new_train_model,test.df),test.df$mpg)
prediction
prediction<-c(predict(new_train_model,test.df),test.df$mpg)
summary(prediction)
prediction<-data.frame(c(predict(new_train_model,test.df),test.df$mpg))
summary(prediction)
prediction<-data.frame(c(predict(new_train_model,test.df),test.df$mpg))
prediction
library("ISLR")
set.seed(1122)
index <- sample(1:nrow(Auto), .95*dim(Auto)[1])
train.df <-Auto[index,]
test.df <-Auto [-index,]
train_model<- lm(mpg ~. -name,data=train.df)
summary(train_model)
RSS<- c(crossprod(train_model$residuals))
MSE <- RSS/ length(train_model$residuals)
RMSE <- sqrt(MSE)
paste("R2 = .8135, RSE =  3.367, RMSE= ", RMSE)
# The model fits the data well because we see that the R2 value is high, which indicates a strong fit. RSE is also low which means that the residual error doesn't vary a lot, and similar with RMSE which means that the average residual error also doesn't vary as much. RSE and RMSE are close to each other which i will infer as it meaning that between the different residuals the average and error don't deviate a lot. Essentially, these are signs of a model with a good fit.
plot(train_model$residuals)
hist(train_model$residuals,main="Histogram of training model residuals", xlab="Residuals")
# The distribution is very close to a Gaussian one. It is very slightly skewed to the left. What I can say about the residuals is that most of them are very close to the mean.
# From part a-ii, the most significant predictors are weight, year, and origin.
new_train_model<- lm(mpg ~ weight+year+origin,data=train.df)
summary(new_train_model)
new_RSS<- c(crossprod(new_train_model$residuals))
new_MSE <- new_RSS/ length(new_train_model$residuals)
new_RMSE <- sqrt(new_MSE)
paste("R2 = .8111, RSE =  3.389, RMSE= ", new_RMSE)
# This model fits the data pretty well as the R2 is still high, the RSE and RMSE are pretty close to 0.
plot(new_train_model$residuals)
hist(new_train_model$residuals,main="Histogram of new training model residuals", xlab="Residuals")
# There is a Gaussian distribution and it looks like this time there are more residuals closer to positive 15. There also seems to be less negative residuals.
prediction<-data.frame(c(predict(new_train_model,test.df),test.df$mpg))
prediction
column1<-c(predict(new_train_model,test.df))
column2<-c(test.df)
prediction<-data.frame(c(column1,column2))
prediction
column1<-c(predict(new_train_model,test.df))
column2<-c(test.df$mpg)
prediction<-data.frame(c(column1,column2))
prediction
column1<-c(predict(new_train_model,test.df))
column2<-c(test.df$mpg)
prediction<-data.frame(column1,column2)
prediction
cbind(Matches,prediction)
sum_test<-apply(prediction$column2,2,sum)
sum_test<-apply(prediction$column1,2,sum)
sum_test<-apply(prediction,2,sum)
sum_test<-apply(prediction,2,sum)
sum_test
sum_test<-apply(prediction$column2,2,sum)
Matches<-c(predict(new_train_model,test.df,interval = 'confidence'))
Matches<-c(predict(new_train_model,test.df,interval = 'confidence'))
Matches
new_column1<-c(predict(new_train_model,test.df,interval = 'confidence'))
new_column2<-c(test.df$mpg)
new_prediction<-data.frame(new_column1,new_column2)
new_prediction
predict(new_train_model,test.df,interval = 'confidence')
new_column2<-c(test.df$mpg)
new_prediction<-data.frame(new_column1,new_column2)
new_prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence')
new_column2<-c(test.df$mpg)
new_prediction<-data.frame(new_column1,new_column2)
new_prediction
new_column1<-c(predict(new_train_model,test.df,interval = 'confidence'))
new_column2<-c(test.df$mpg)
new_prediction<-data.frame(new_column1,new_column2)
new_prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence')
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction
column1<-predict(new_train_model,test.df)
column2<-test.df$mpg
prediction<-data.frame(column1,column2)
prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
check_c_level<- if (new_prediction$new_column2 <= new_prediction$upr && new_prediction$new_column2 >= new_prediction$lwr)
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
ifelse(new_prediction$new_column2 <= new_prediction$upr && new_prediction$new_column2 >= new_prediction$lwr,1,0)
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr && new_prediction$new_column2 >= new_prediction$lwr,1,0))
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr && new_prediction$new_column2 >= new_prediction$lwr,1,0))
Matches
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr && new_prediction$new_column2 >= new_prediction$lwr,1,0))
Matches
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr && new_prediction$new_column2 >= new_prediction$lwr,1,0))
new_prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr && new_prediction$new_column2 >= new_prediction$lwr,0,1))
new_prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr && new_prediction$new_column2 >= new_prediction$lwr,1,0))
new_prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
new_prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
new_prediction
Num_Matches<-apply(new_prediction$Matches,2,sum)
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
new_prediction
Num_Matches<-apply(new_prediction$Matches,1,sum)
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
new_prediction
Num_Matches<-apply(new_prediction["Matches"],2,sum)
Num_Matches
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
Num_Matches<-apply(new_prediction["Matches"],2,sum)
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
Num_Matches<-apply(new_prediction["Matches"],2,sum)
new_prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
Num_Matches<-apply(new_prediction["Matches"],2,sum)
new_prediction
paste("Total Observations corrextly predicted:", Num_Matches )
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
Num_Matches<-apply(new_prediction["Matches"],2,sum)
new_prediction
paste("Total Observations correctly predicted:", Num_Matches )
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
Num_Matches<-apply(new_prediction["Matches"],2,sum)
new_prediction
paste("Total Observations correctly predicted:", Num_Matches)
newer_column1<-predict(new_train_model,test.df,interval = 'prediction',level=.95)
newer_column2<-test.df$mpg
newer_prediction<-data.frame(newer_column1,newer_column2)
newer_prediction$Matches<-c(ifelse(newer_prediction$newer_column2 <= newer_prediction$upr & newer_prediction$newer_column2 >= newer_prediction$lwr,1,0))
Number_Matches<-apply(newer_prediction["Matches"],2,sum)
newer_prediction
paste("Total Observations correctly predicted:", Number_Matches)
library("ISLR")
set.seed(1122)
index <- sample(1:nrow(Auto), .95*dim(Auto)[1])
train.df <-Auto[index,]
test.df <-Auto [-index,]
train_model<- lm(mpg ~. -name,data=train.df)
summary(train_model)
RSS<- c(crossprod(train_model$residuals))
MSE <- RSS/ length(train_model$residuals)
RMSE <- sqrt(MSE)
paste("R2 = .8135, RSE =  3.367, RMSE= ", RMSE)
# The model fits the data well because we see that the R2 value is high, which indicates a strong fit. RSE is also low which means that the residual error doesn't vary a lot, and similar with RMSE which means that the average residual error also doesn't vary as much. RSE and RMSE are close to each other which i will infer as it meaning that between the different residuals the average and error don't deviate a lot. Essentially, these are signs of a model with a good fit.
plot(train_model$residuals)
hist(train_model$residuals,main="Histogram of training model residuals", xlab="Residuals")
# The distribution is very close to a Gaussian one. It is very slightly skewed to the left. What I can say about the residuals is that most of them are very close to the mean.
# From part a-ii, the most significant predictors are weight, year, and origin.
new_train_model<- lm(mpg ~ weight+year+origin,data=train.df)
summary(new_train_model)
new_RSS<- c(crossprod(new_train_model$residuals))
new_MSE <- new_RSS/ length(new_train_model$residuals)
new_RMSE <- sqrt(new_MSE)
paste("R2 = .8111, RSE =  3.389, RMSE= ", new_RMSE)
# This model fits the data pretty well as the R2 is still high, the RSE and RMSE are pretty close to 0.
plot(new_train_model$residuals)
hist(new_train_model$residuals,main="Histogram of new training model residuals", xlab="Residuals")
# There is a Gaussian distribution and it looks like this time there are more residuals closer to positive 15. There also seems to be less negative residuals.
column1<-predict(new_train_model,test.df)
column2<-test.df$mpg
prediction<-data.frame(column1,column2)
prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
Num_Matches<-apply(new_prediction["Matches"],2,sum)
new_prediction
paste("Total Observations correctly predicted:", Num_Matches)
newer_column1<-predict(new_train_model,test.df,interval = 'prediction',level=.95)
newer_column2<-test.df$mpg
newer_prediction<-data.frame(newer_column1,newer_column2)
newer_prediction$Matches<-c(ifelse(newer_prediction$newer_column2 <= newer_prediction$upr & newer_prediction$newer_column2 >= newer_prediction$lwr,1,0))
Number_Matches<-apply(newer_prediction["Matches"],2,sum)
newer_prediction
paste("Total Observations correctly predicted:", Number_Matches)
plot(train_model$residuals,1)
plot(train_model$residuals)
plot(train_model,1)
plot(new_train_model,1)
library("ISLR")
set.seed(1122)
index <- sample(1:nrow(Auto), .95*dim(Auto)[1])
train.df <-Auto[index,]
test.df <-Auto [-index,]
train_model<- lm(mpg ~. -name,data=train.df)
summary(train_model)
RSS<- c(crossprod(train_model$residuals))
MSE <- RSS/ length(train_model$residuals)
RMSE <- sqrt(MSE)
paste("R2 = .8135, RSE =  3.367, RMSE= ", RMSE)
# The model fits the data well because we see that the R2 value is high, which indicates a strong fit. RSE is also low which means that the residual error doesn't vary a lot, and similar with RMSE which means that the average residual error also doesn't vary as much. RSE and RMSE are close to each other which i will infer as it meaning that between the different residuals the average and error don't deviate a lot. Essentially, these are signs of a model with a good fit.
plot(train_model,1)
hist(train_model$residuals,main="Histogram of training model residuals", xlab="Residuals")
# The distribution is very close to a Gaussian one. It is very slightly skewed to the left. What I can say about the residuals is that most of them are very close to the mean.
# From part a-ii, the most significant predictors are weight, year, and origin.
new_train_model<- lm(mpg ~ weight+year+origin,data=train.df)
summary(new_train_model)
new_RSS<- c(crossprod(new_train_model$residuals))
new_MSE <- new_RSS/ length(new_train_model$residuals)
new_RMSE <- sqrt(new_MSE)
paste("R2 = .8111, RSE =  3.389, RMSE= ", new_RMSE)
# This model fits the data pretty well as the R2 is still high, the RSE and RMSE are pretty close to 0.
plot(new_train_model,1)
hist(new_train_model$residuals,main="Histogram of new training model residuals", xlab="Residuals")
# There is a Gaussian distribution and it looks like this time there are more residuals closer to positive 15. There also seems to be less negative residuals.
column1<-predict(new_train_model,test.df)
column2<-test.df$mpg
prediction<-data.frame(column1,column2)
prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
Num_Matches<-apply(new_prediction["Matches"],2,sum)
new_prediction
paste("Total Observations correctly predicted:", Num_Matches)
newer_column1<-predict(new_train_model,test.df,interval = 'prediction',level=.95)
newer_column2<-test.df$mpg
newer_prediction<-data.frame(newer_column1,newer_column2)
newer_prediction$Matches<-c(ifelse(newer_prediction$newer_column2 <= newer_prediction$upr & newer_prediction$newer_column2 >= newer_prediction$lwr,1,0))
Number_Matches<-apply(newer_prediction["Matches"],2,sum)
newer_prediction
paste("Total Observations correctly predicted:", Number_Matches)
df<- read.csv(file="us-covid-deaths.csv")
df_complete<- df[complete.cases(df), ]
df_complete [1:6,]
library("psych")
pairs.panels(df_complete)
covid_regress_model<-lm( total_deaths ~ . - date, data=df_complete)
summary(covid_regress_model)
rss<-c(crossprod(covid_regress_model$residuals))
df_removed_predictor<-df_complete[,c(-1,-7)]
summary(lm(df_removed_predictor))
# This model seems to be worse than the one from part 2.1-C because the F-statistic is much lower and the adjusted R^2 value is about 33% lower as well.
covid_regress_model<-lm( total_deaths ~ . - date, data=df_complete)
summary(covid_regress_model)
rss<-c(crossprod(covid_regress_model$residuals))
rss
covid_regress_model<-lm( total_deaths ~ . - date, data=df_complete)
summary(covid_regress_model)
rss<-c(crossprod(covid_regress_model))
covid_regress_model<-lm( total_deaths ~ . - date, data=df_complete)
summary(covid_regress_model)
rss<-c(crossprod(covid_regress_model$residuals))
covid_regress_model<-lm( total_deaths ~ . - date, data=df_complete)
summary(covid_regress_model)
rss<-c(crossprod(covid_regress_model$residuals))
rss
df_removed_predictor<-df_complete[,c(-1,-7)]
summary(lm(df_removed_predictor))
c(crossprod(df_removed_predictor$residuals))
df_removed_predictor<-df_complete[,c(-1,-7)]
g<lm(df_removed_predictor)
df_removed_predictor<-df_complete[,c(-1,-7)]
g<-lm(df_removed_predictor)
grss<-c(crossprod(g$residuals))
# This model seems to be worse than the one from part 2.1-C because the F-statistic is much lower and the adjusted R^2 value is about 33% lower as well.
df_removed_predictor<-df_complete[,c(-1,-7)]
g<-lm(df_removed_predictor)
grss<-c(crossprod(g$residuals))
grss
# This model seems to be worse than the one from part 2.1-C because the F-statistic is much lower and the adjusted R^2 value is about 33% lower as well.
df_removed_predictor<-df_complete[,c(-1,-7)]
g<-lm(df_removed_predictor)
RSS(g)
df_removed_predictor<-df_complete[,c(-1,-7)]
g<-lm(df_removed_predictor)
plot(residuals(g), main="Residuals")
grss<-c(crossprod(g$residuals))
grss
# This model seems to be worse than the one from part 2.1-C because the F-statistic is much lower and the adjusted R^2 value is about 33% lower as well.
covid_regress_model<-lm( total_deaths ~ . - date, data=df_complete)
plot(residuals(covid_regress_model), main="Residuals")
summary(covid_regress_model)
rss<-c(crossprod(covid_regress_model$residuals))
rss
install.packages('qpcR')
library(qpcR)
RSS(g)
covid_regress_model<-lm( total_deaths ~ . - date, data=df_complete)
plot(residuals(covid_regress_model), main="Residuals")
summary(covid_regress_model)
rss<-c(crossprod(covid_regress_model$residuals))
rss
plot(residuals(train_model), main="Residuals")
plot(residuals(train_model), main="Residuals")
plot(residuals(train_model), main="Residuals of training data")
plot(residuals(new_train_model), main="Residuals for updated model")
library("ISLR")
set.seed(1122)
index <- sample(1:nrow(Auto), .95*dim(Auto)[1])
train.df <-Auto[index,]
test.df <-Auto [-index,]
train_model<- lm(mpg ~. -name,data=train.df)
summary(train_model)
RSS<- c(crossprod(train_model$residuals))
MSE <- RSS/ length(train_model$residuals)
RMSE <- sqrt(MSE)
paste("R2 = .8135, RSE =  3.367, RMSE= ", RMSE)
# The model fits the data well because we see that the R2 value is high, which indicates a strong fit. RSE is also low which means that the residual error doesn't vary a lot, and similar with RMSE which means that the average residual error also doesn't vary as much. RSE and RMSE are close to each other which i will infer as it meaning that between the different residuals the average and error don't deviate a lot. Essentially, these are signs of a model with a good fit.
plot(residuals(train_model), main="Residuals of training data")
hist(train_model$residuals,main="Histogram of training model residuals", xlab="Residuals")
# The distribution is very close to a Gaussian one. It is very slightly skewed to the left. What I can say about the residuals is that most of them are very close to the mean.
# From part a-ii, the most significant predictors are weight, year, and origin.
new_train_model<- lm(mpg ~ weight+year+origin,data=train.df)
summary(new_train_model)
new_RSS<- c(crossprod(new_train_model$residuals))
new_MSE <- new_RSS/ length(new_train_model$residuals)
new_RMSE <- sqrt(new_MSE)
paste("R2 = .8111, RSE =  3.389, RMSE= ", new_RMSE)
# This model fits the data pretty well as the R2 is still high, the RSE and RMSE are pretty close to 0.
plot(residuals(new_train_model), main="Residuals for updated model")
hist(new_train_model$residuals,main="Histogram of new training model residuals", xlab="Residuals")
# There is a Gaussian distribution and it looks like this time there are more residuals closer to positive 15. There also seems to be less negative residuals.
column1<-predict(new_train_model,test.df)
column2<-test.df$mpg
prediction<-data.frame(column1,column2)
prediction
new_column1<-predict(new_train_model,test.df,interval = 'confidence',level=.95)
new_column2<-test.df$mpg
new_prediction<-data.frame(new_column1,new_column2)
new_prediction$Matches<-c(ifelse(new_prediction$new_column2 <= new_prediction$upr & new_prediction$new_column2 >= new_prediction$lwr,1,0))
Num_Matches<-apply(new_prediction["Matches"],2,sum)
new_prediction
paste("Total Observations correctly predicted:", Num_Matches)
newer_column1<-predict(new_train_model,test.df,interval = 'prediction',level=.95)
newer_column2<-test.df$mpg
newer_prediction<-data.frame(newer_column1,newer_column2)
newer_prediction$Matches<-c(ifelse(newer_prediction$newer_column2 <= newer_prediction$upr & newer_prediction$newer_column2 >= newer_prediction$lwr,1,0))
Number_Matches<-apply(newer_prediction["Matches"],2,sum)
newer_prediction
paste("Total Observations correctly predicted:", Number_Matches)
setwd("~/RStuff/HW4")
install.packages("rpart")
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
?rpart
?rpart.plot
fit<-rpart(data=iris)
?rpart
library(rpart)
library(rpart.plot)
model<-rpart(method="class",data=iris)
iris
library(rpart)
library(rpart.plot)
model<-rpart(Species~,method="class",data=iris)
library(rpart)
library(rpart.plot)
model<-rpart(Species~ method="class",data=iris)
library(rpart)
library(rpart.plot)
model<-rpart(Species~. method="class",data=iris)
library(rpart)
library(rpart.plot)
model<-rpart(Species~., method="class",data=iris)
rpart.plot(model, extra=104, fallen.leaves=T, type=4, main="Iris Dataset Decision Tree")
library(rpart)
library(rpart.plot)
model<-rpart(Species~., method="class",data=iris)
model-plot<-rpart.plot(model, extra=104, fallen.leaves=T, type=4, main="Iris Dataset Decision Tree")
library(rpart)
library(rpart.plot)
model<-rpart(Species~., method="class",data=iris)
rpart.plot(model, extra=104, fallen.leaves=T, type=4, main="Iris Dataset Decision Tree")
class(model)
class(model)
summary(model)
?rpart
